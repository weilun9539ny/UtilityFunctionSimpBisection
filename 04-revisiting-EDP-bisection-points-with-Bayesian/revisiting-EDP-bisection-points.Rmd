---
title: "Revisiting EDP Data Points"
author: "Wei Lun, Lin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(tidyverse)
library(cmdstanr)
library(posterior)
library(bayesplot)
```

## Overview

We are curious about the comparison between the two estimation methods:

-   least-square
-   Hierarchical Bayesian
-   Bayesian

## EDP data

We load the data of 87 subjects. The data contains $[x^-_8, ..., x^-_1, x_1^+, ..., x_8^+]$ and the $\alpha, \beta$ estimated with lest square method.

```{r read-data}
dat <- read.csv("../01-data/raw_data/EDP_stage1_rawdata.csv")
dat %>% head(10)
```

Our model is

$$
U(x)=
\begin{cases}
  x^\alpha, & x \geq 0  \\
  -(-x)^\beta, & x < 0
\end{cases}
$$

We fitted the model to the bisection data $[x^-_8, x_7^-, ..., x^-_1, x^+_1, x^+_2, ..., x^+_8]$, where the relationship between the data and the utilities is

|  |  |  |  |  |  |  |  |  |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| $x$ | $x^-_8$ | $x^-_7$ | $x^-_6$ | $x^-_5$ | $x^-_4$ | $x^-_3$ | $x^-_2$ | $x^-_1$ |
| $U(x)$ | $-{8\over8}$ | $-{7\over8}$ | $-{6\over8}$ | $-{5\over8}$ | $-{4\over8}$ | $-{3\over8}$ | $-{2\over8}$ | $-{1\over8}$ |

|  |  |  |  |  |  |  |  |  |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| $x$ | $x^+_1$ | $x^+_2$ | $x^+_3$ | $x^+_4$ | $x^+_5$ | $x^+_6$ | $x^+_7$ | $x^+_8$ |
| $U(x)$ | ${1\over8}$ | ${2\over8}$ | ${3\over8}$ | ${4\over8}$ | ${5\over8}$ | ${6\over8}$ | ${7\over8}$ | ${8\over8}$ |

Next, we rearrange the data frame so that the columns are `subject_id`, `subject`, `x`, and `y`, where `x` is normalized $x$ data points and `y` is normalized utility $u$.

```{r rearrange-data}
dat.long <- dat %>% 
  select(-(19:23)) %>%   # select subject id and Xs
  pivot_longer(starts_with("x"), names_to = "x.type", values_to = "x") %>% 
  mutate(
    subject = as.factor(subject),
    sign.x = sign(x)
  ) %>% 
  group_by(subject, sign.x) %>% 
  mutate(
    x = x / max(abs(x)),
    y = seq(1/8, 1, 1/8) * sign.x
  ) %>% 
  ungroup() %>% 
  select(-sign.x) %>%
  rename(subj_id = index)

dat.long %>% head(10)
```


## Hierarchical Bayesian Estimation

We specified the Stan model for utility value function:

$$
y=
\begin{cases}
  x^\alpha, & x \geq0  \\
  -(-x)^\beta, & \text{otherwise}
\end{cases}
$$

We specified the input data for model.

```{r mcmc-data-prepare, cache=T}
dat.pos <- dat.long %>% 
  filter(x >= 0)
dat.neg <- dat.long %>%
  filter(x < 0) %>% 
  mutate(x = -x)  # abs(-x) so that y=-(-x)^beta can be computed faster

J <- max(dat.long$subj_id)

dat.list <- list(
  J = J,
  N = nrow(dat.pos),
  N_subj = dat.pos$subj_id,
  x_pos = dat.pos$x,
  y_pos = dat.pos$y,
  x_neg = dat.neg$x,
  y_neg = -dat.neg$y
)
```



```{r MCMC}
value_func.model <- cmdstan_model("./model/power_value_func_y-fixed.stan")
value_func.model$check_syntax()

value_func.fit <- value_func.model$sample(
  data = dat.list,
  seed = 1024,
  refresh = 1e3,
  chains = 4,
  parallel_chains = 4,
  iter_warmup = 1e3,
  iter_sampling = 6e3
)
```

The fitting result is listed below.

```{r mcmc-summary}
param <- c(
  # "mu_log_alpha_inverse", "mu_log_beta", "tau_log_alpha", "tau_log_beta", "sigma",
  paste0("alpha[", 1:J, "]"), paste0("beta[", 1:J, "]")
)
value_func.summary <- value_func.fit$summary(variables = param)
value_func.summary
```

Besides, we stored the fitting results.

```{r save-fitting-results}
value_func.draw <- value_func.fit$draws(param)

saveRDS(value_func.fit, "./result/fitting/fit.rds")
saveRDS(value_func.summary, "./result/fitting/summary.rds")
saveRDS(value_func.draw, "./result/fitting/draws.rds")
```

After model fitting, we checked trace plots of parameters to make sure if the MCs converged.

```{r trace-plot, warning=F, message=F, cache=T}
trace.plot.path <- "./result/trace-plot/"
save.plot <- function(plot.type=1, name) {
  if (plot.type == 1) dir.path <- "./result/trace-plot/"
  else if (plot.type == 2) dir.path <- "./result/posterior/"
  else if (plot.type == 3) dir.path <- "./result/interval/"
  else if (plot.type == 4) dir.path <- "./result/subject-plot/"
  else {
    warning("Plot type not assigned, plot not saved!")
    return()
  }
  file.name <- paste0(dir.path, paste(name, collapse="-"), ".png")
  ggsave(file.name)
}

# value_func.draw <- readRDS("./result/fitting/draws.rds")
# group-level parameters
# mcmc_trace(value_func.draw, pars=param[1:5]) +
  # labs(title = "Trace Plot of Group-Level Parameters") +
  # scale_x_continuous(
    # name = "Number of iterations (1000)",
    # labels = scales::label_comma(scale = 1/1000)
  # ) +
  # theme_light()
# save.plot(1, "trace_plot-grp_param")

# individual alphas
for (j in 1:5) {
  par.index <- na.omit((1:87)[(18 * j - 17):(18 * j)])
  par.range <- range(par.index)
  trace.plot <- mcmc_trace(value_func.draw, pars=param[par.index]) +
    scale_x_continuous(
      name = "Number of iterations (1000)",
      labels = scales::label_comma(scale = 1/1000)
    ) +
    labs(title = paste(
      "Trace Plot of Individual Alpha: Subject", 
      par.range[1], "~", par.range[2]
    )) +
    theme_light()
  print(trace.plot)
  save.plot(1, name=c("trace_plot-ind_alpha", par.range[1], par.range[2], "y-fixed"))
}

# individual betas
for (j in 1:5) {
  par.index <- na.omit((1:87)[(18 * j - 17):(18 * j)] + 87)
  par.range <- range(par.index) - 87
  trace.plot <- mcmc_trace(value_func.draw, pars=param[par.index]) +
    scale_x_continuous(
      name = "Number of iterations (1000)",
      labels = scales::label_comma(scale = 1/1000)
    ) +
    labs(title = paste(
      "Trace Plot of Individual Beta: Subject", 
      par.range[1], "~", par.range[2]
    )) +
    theme_light()
  print(trace.plot)
  save.plot(1, name=c("trace_plot-ind_beta", par.range[1], par.range[2], "y-fixed"))
}

rm(trace.plot)
```

And we checked if the posterior distribution of each parameters at the 4 chains are similar.

```{r posterior-dist, cache=T}
# group-level
# mcmc_dens_overlay(value_func.draw, pars = param[1:5]) +
  # labs(title = "Posterior Distributions of Group-Level Parameters") +
  # theme_light()
# save.plot(2, c("posterior", "grp_param"))

# individual alphas
for (j in 1:5) {
  par.index <- na.omit((1:87)[(18 * j - 17):(18 * j)])
  par.range <- range(par.index)
  post.dens <- mcmc_dens_overlay(value_func.draw, pars=param[par.index]) +
    labs(title = paste(
      "Posterior Distributions of Individual Alpha: Subject", 
      par.range[1], "~", par.range[2]
    )) +
    theme_light()
  print(post.dens)
  save.plot(2, name=c("posterior", "ind_alpha", par.range[1], par.range[2], "y-fixed"))
}

# individual betas
for (j in 1:5) {
  par.index <- na.omit((1:87)[(18 * j - 17):(18 * j)] + 87)
  par.range <- range(par.index) - 87
  post.dens <- mcmc_dens_overlay(value_func.draw, pars=param[par.index]) +
    labs(title = paste(
      "Posterior Distributions of Individual Beta: Subject", 
      par.range[1], "~", par.range[2]
    )) +
    theme_light()
  print(post.dens)
  save.plot(2, name=c("posterior", "ind_beta", par.range[1], par.range[2], "y-fixed"))
}

rm(post.dens)
```

Next, we checked the minimum $\hat R$ and effective sample size.

```{r min-rhat-ess}
value_func.summary %>% 
  select(c(variable, rhat, starts_with("ess"))) %>% 
  # filter(rhat == max(rhat))
  # filter(ess_bulk == min(ess_bulk))
  # filter(ess_tail == min(ess_tail))
```


Finally, we store the posterior mean and median to a data frame.

```{r bayes-result}
res.df <- dat %>% 
  transmute(
    subj_id = index,
    subject = subject,
    alpha.index = paste0("alpha[", index, "]"),
    beta.index = paste0("beta[", index, "]"),
    alpha.bayes.mean = value_func.summary[1:87, 2][[1]],
    beta.bayes.mean = value_func.summary[88:174, 2][[1]],
    alpha.bayes.median = value_func.summary[1:87, 3][[1]],
    beta.bayes.median = value_func.summary[88:174, 3][[1]],
    alpha.nls = NA,
    beta.nls = NA
  )

res.df %>% head(10)
```


## Re-fit Non-Linear Power

Since $x$ are fixed by the experiment design, the loss function should be

$$
\sum_{i=1}^{16}(x_i - y_i^{1/\alpha}).
$$

```{r nls}
for (j in dat$index) {
  # fit non-linear least square
  dat.pos.cur <- dat.pos %>% 
    filter(subj_id == j)
  dat.neg.cur <- dat.neg %>% 
    filter(subj_id == j)
  # res.alpha <- nls(y ~ x^alpha, dat.pos.cur, start=list(alpha=0)) %>% coef(F)  # x fixed
  res.alpha <- nls(
    x ~ y^(1/alpha), dat.pos.cur, 
    start=list(alpha=.01), control=list("maxiter"=100)
  ) %>% coef(F)  # y fixed
  # res.beta <- nls(y ~ -x^beta, dat.neg.cur, start=list(beta=0)) %>% coef(F)  # x fixed
  res.beta <- nls(
    x ~ (-y)^(1/beta), dat.neg.cur, 
    start=list(beta=.01), control=list("maxiter"=100)
  ) %>% coef(F)  # y fixed
  
  # store the result
  res.df[j, 9:10] <- c(res.alpha, res.beta)
}

rm(dat.pos.cur, dat.neg.cur, res.alpha, res.beta)

sum(is.na(res.df$alpha.nls), is.na(res.df$beta.nls))  # 0: no na, all fitted

# res.df %>% write.csv("./result/estimation-nls-HB_y-fixed.csv", row.names = F)
res.df %>% head(10)
```


## Comparison Between LSE and HBE

We plot the interval estimations of hierarchical Bayesian, and add the nls estimation on the plot, to compare the estimations of non-linear least square and hierarchical bayesian.

```{r compare-intervals, cache=T}
# group-level
# mcmc_intervals(
#   value_func.draw, pars = param[1:5], 
#   point_est="mean", prob=.68, prob_outer = .95
# ) +
#   labs(title = "Interval Estimations of Group-level Parameters") +
#   theme_light()
# save.plot(3, c("interval", "grp_param"))

# individual alphas
for (j in 1:3) {
  par.index <- na.omit((1:87)[(29 * j - 28):(29 * j)])
  par.range <- range(par.index)
  inter.est <- mcmc_intervals(
    value_func.draw, pars = param[par.index], 
    point_est="mean", prob=.68, prob_outer = .95
  ) +
    geom_point(
      data = res.df[par.index, ],
      aes(x=alpha.nls, y=alpha.index),
      color = "pink", size=3, alpha=.7
    ) +
    labs(title = paste(
      "Interval Estimations of Individual Alpha: Subject", 
      par.range[1], "~", par.range[2]
    )) +
    theme_light()
  print(inter.est)
  save.plot(3, name=c("interval", "ind_alpha", par.range[1], par.range[2], "y-fixed"))
}

# individual betas
for (j in 1:3) {
  par.index <- na.omit((1:87)[(29 * j - 28):(29 * j)] + 87)
  par.range <- range(par.index) - 87
  inter.est <- mcmc_intervals(
    value_func.draw, pars = param[par.index], 
    point_est="mean", prob=.68, prob_outer = .95
  ) +
    geom_point(
      data = res.df[par.index - 87, ],
      aes(x=beta.nls, y=beta.index),
      color = "pink", size=3, alpha=.7
    ) +
    labs(title = paste(
      "Interval Estimations of Individual Beta: Subject", 
      par.range[1], "~", par.range[2]
    )) +
    theme_light()
  print(inter.est)
  save.plot(3, name=c("interval", "ind_beta", par.range[1], par.range[2], "y-fixed"))
}

rm(inter.est)
```

And we use scatter plot to compare the point estimation of frequentist and hierarchical Bayesian posterior mean.

```{r compare-scatter, cache=T}
res.df %>% 
  # filter(alpha.nls < 2) %>%
  ggplot(aes(x=alpha.nls, y=alpha.bayes.mean)) +
  geom_point() +
  geom_abline(intercept=0, slope=1, color="red", linewidth=1, alpha=.5) +
  # stat_smooth(
    # method="lm", se=F, alpha=.5
  # ) +
  annotate("text", x=3, y=3.5, color="red", label="y = x") +
  # annotate("text", x=3, y=2, color="blue", label="lm(y ~ x)") +
  # lims(x = c(0, 5.5), y=c(0, 5.5)) +
  labs(
    title = "Scatter Plot of Bayesian (Mean) vs. Frequentist Alpha Estimates",
    x = "Frequentist Estimation",
    y = "Hierarchical Bayesian Estimation (Posterior Mean)"
  ) +
  theme_light()
ggsave("./result/compare/compare-alpha-post_mean_nls.png")

res.df %>% 
  # filter(alpha.nls < 2) %>%
  ggplot(aes(x=alpha.nls, y=alpha.bayes.median)) +
  geom_point() +
  geom_abline(intercept=0, slope=1, color="red", linewidth=1, alpha=.5) +
  annotate("text", x=3, y=3.5, color="red", label="y = x") +
  labs(
    title = "Scatter Plot of Bayesian (Median) vs. Frequentist Alpha Estimates",
    x = "Frequentist Estimation",
    y = "Hierarchical Bayesian Estimation (Posterior Median)"
  ) +
  theme_light()
ggsave("./result/compare/compare-alpha-post_median_nls.png")

res.df %>% 
  # filter(beta.nls < 10) %>%
  ggplot(aes(x=beta.nls, y=beta.bayes.mean)) +
  geom_point() +
  geom_abline(intercept=0, slope=1, color="red", linewidth=1, alpha=.5) +
  # stat_smooth(
    # data=filter(point.est, freq.alpha<20),
    # method="lm", se=F, alpha=.5
  # ) +
  annotate("text", x=2, y=2.3, color="red", label="y = x") +
  # annotate("text", x=2, y=1.3, color="blue", label="lm(y ~ x)") +
  labs(
    title = "Scatter Plot of Bayesian (Mean) vs. Frequentist Beta Estimates",
    # subtitle = "Outliers of frequentist beta are excluded",
    x = "Frequentist Estimation",
    y = "Hierarchical Bayesian Estimation (Posterior Mean)"
  ) +
  theme_light()
ggsave("./result/compare/compare-beta-post_mean_nls.png")

res.df %>% 
  # filter(beta.nls < 10) %>%
  ggplot(aes(x=beta.nls, y=beta.bayes.median)) +
  geom_point() +
  geom_abline(intercept=0, slope=1, color="red", linewidth=1, alpha=.5) +
  # stat_smooth(
    # data=filter(point.est, freq.alpha<20),
    # method="lm", se=F, alpha=.5
  # ) +
  annotate("text", x=2, y=2.3, color="red", label="y = x") +
  # annotate("text", x=2, y=1.3, color="blue", label="lm(y ~ x)") +
  labs(
    title = "Scatter Plot of Bayesian (Median) vs. Frequentist Beta Estimates",
    # subtitle = "Outliers of frequentist beta are excluded",
    x = "Frequentist Estimation",
    y = "Hierarchical Bayesian Estimation (Posterior Median)"
  ) +
  theme_light()
ggsave("./result/compare/compare-beta-post_median_nls.png")
```

Next, we plot each subjects' $x^-_8, ..., x^+_8$ and the two fitting results (non-linear least square and hierarchical Bayesian).

```{r plot-point-and-models, cache=T}
for (i in res.df$subj_id) {
  par <- c(
    res.df$alpha.nls[i], res.df$beta.nls[i],
    res.df$alpha.bayes.mean[i], res.df$beta.bayes.mean[i],
    res.df$alpha.bayes.median[i], res.df$beta.bayes.median[i]
  )
  label <- c(
    paste("alpha (nls):", round(par[1], 3)),
    paste("beta (nls):", round(par[2], 3)),
    paste("alpha (post mean):", round(par[3], 3)),
    paste("beta (post mean):", round(par[4], 3)),
    paste("alpha (post median):", round(par[5], 3)),
    paste("beta (post median):", round(par[6], 3))
  )
  (subj.plot <- dat.long %>% 
    filter(subj_id == i) %>% 
    ggplot(aes(x=x, y=y)) +
    geom_point() +
    geom_function(  # alpha.nls
      fun = \(x) x^par[1],
      aes(color = label[1]),
      # color="red",
      alpha=.5, xlim = c(0, 1)
    ) +
    geom_function(  # beta.nls
      fun = \(x) -(-x)^par[2],
      aes(color = label[2]),
      # color="red",
      alpha=.5, xlim = c(-1, 0)
    ) +
    geom_function(  # alpha.bayes.mean
      fun = \(x) x^par[3],
      aes(color = label[3]),
      # color="blue",
      alpha=.5, xlim = c(0, 1)
    ) +
    geom_function(  # beta.bayes.mean
      fun = \(x) -(-x)^par[4],
      aes(color = label[4]),
      # color="blue",
      alpha=.5, xlim = c(-1, 0)
    ) +
    geom_function(  # alpha.bayes.median
      fun = \(x) x^par[5],
      aes(color = label[5]),
      # color="blue",
      alpha=.5, xlim = c(0, 1)
    ) +
    geom_function(  # beta.bayes.mean
      fun = \(x) -(-x)^par[6],
      aes(color = label[6]),
      # color="blue",
      alpha=.5, xlim = c(-1, 0)
    ) +
    labs(
      title = paste("Subject:", res.df$subject[i]),
      y="U(x)"
    ) +
    # define color
    scale_colour_manual(
      "parameters",
      # values = c("red", "red", "blue", "blue", "green", "green")
      values = rep(c("red", "blue", "green"), 2)
    ) +
    theme_light())
  # print(subj.plot)
  save.plot(4, c("subject_plot", res.df$subject[i], "y_fixed"))
}

rm(subj.plot)
```

