---
title: "Stage 1 Utility Function Estimation"
author: "Wei-Lun, Lin"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

library(rstan)
library(bayesplot)
library(posterior)
# library(loo)
```


## Data Reading and Processing

We first get raw data from folders of 2021, 2022 and 2023.
The format of the data in 2021 is a little different from those in 2022 and 2023, 
so we need to separate different cases.

```{r get rawdata}
# get rawdata from folders of 2021-2023
rawdata <- data.frame()
files.dir <- list.files("./data/raw_data", full.names=T)[-16]
for (i in 1:length(files.dir)) {
  path <- paste0(files.dir[i], "/rawData")
  files <- list.files(path, pattern=".csv", full.names=T)
  for (file in files) {
    if (i < 7) sub.data <- read.csv(file)[, 1:247]
    else sub.data <- read.csv(file)[, 2:248]
    sub <- str_split(file, "/")[[1]][5] %>% substr(1, 10)
    sub.data$sub <- sub
    # print(sub)
    rawdata <- rbind(rawdata, sub.data)
  }
}
rm(i, sub.data, file, files, files.dir, path, sub)

dim(rawdata)
head(rawdata[, 1:5])
# write.csv(rawdata, "./rawdata.csv", row.names=F)
```

We have 165 subjects in total.

The extracted raw data contains action time points, values and choices.
To note that, we do not need the time point data, so we remove them next.

```{r remove RT and add subj id}
df <- read.csv("./data/preprocessed_data/rawdata.csv")
col.target <- c(
  seq(3, 247, 13), 
  seq(4, 247, 13),
  seq(13, 247, 13)
) %>% 
  sort()
colnames(df)[248] <- "subject"
df <- df[, c(248, col.target)]
df <- df %>% 
  mutate(
    subj_id = row_number()
  ) %>% 
  relocate(subj_id)
rm(col.target)

head(df[, 1:5])
```

As the part of the data showed above, there are several problems about the content of the data.

 + Columns contain strings of list structure in Python. For example, a possible element could be `"[1000, 800, 600, 400, 200, 100, None, None, 100]"` or `"['A', 'B', 'B', 'A', 'A']"`. These strings should be convert to vectors.
 + There are elements in lists that should be discard. For example, the 6th to 9th elements in value list have no corresponding choice data.
 + The structure of the data frame should allow us to easily analysis the data. For example, each trial should be in a single row.
 + The raw data contained only the value being measured, not all the outcomes of lotteries. Thus, the outcomes should be find out according to the experiment setting.
 + Some subjects did not react to the lotteries in some trials, so there are no choice data in these trials. These trials should be discarded.

To convert the strings to value vectors, I used the `fromJSON()` function in package `jsonlite` to parse the string of list, and convert elements into vector.
Besides, the function automatically detected the type of the elements, which was convenient.

To discard some elements that will not be used, I simply take the top 5 elements in value vectors.

To add additional experiment settings, I took value lists and added other outcomes manually row-wise.

```{r to trial structure}
parse_list_string <- function(x) {
  # replace '' and None
  x <- gsub("None", "null", x)
  x <- gsub("'", '"', x)
  # parse the string
  res <- jsonlite::fromJSON(x)
  res <- if(length(res) == 0) NA else res
  return(res)
}

remove_excess_info <- function(vec) {
  vec[1:5]
}

trial_seq <- Vectorize(seq.default, vectorize.args = c("from", "to"))
pos_seq <- trial_seq(from=26:30, to=91:95, by=10) %>% as.vector %>% sort
neg_seq <- trial_seq(from=31:35, to=91:95, by=10) %>% as.vector %>% sort

df_long <- df %>%
  mutate(
    across(-c(subj_id, subject), ~map(.x, parse_list_string)),  # make python list into R vector
    across(ends_with("_list"), ~map(.x, remove_excess_info))  # make value list length be 5
  ) %>% 
  rowwise() %>% 
  transmute(  # add lottery outcomes
    subj_id = subj_id,
    subject = subject,
    trial = list(c(1:25, pos_seq, neg_seq)),
    xA1 = list(c(
      rep(2000, 10), rep(0, 10), rep(300, 5), 
      rep(x1pos[[1]], 5), rep(x2pos[[1]], 5), rep(x3pos[[1]], 5), rep(x4pos[[1]], 5), rep(x5pos[[1]], 5), rep(x6pos[[1]], 5), rep(x7pos[[1]], 5),
      rep(300, 35)
    )),
    xA2 = list(c(
      L_list, rep(0, 5), rep(L[[1]], 5), rep(-300, 5), rep(0, 5), 
      rep(-300, 35),
      rep(x1neg[[1]], 5), rep(x2neg[[1]], 5), rep(x3neg[[1]], 5), rep(x4neg[[1]], 5), rep(x5neg[[1]], 5), rep(x6neg[[1]], 5), rep(x7neg[[1]], 5)
    )),
    xB1 = list(c(
      rep(0, 5), x1pos_list, x1neg_list, rep(x1pos[[1]], 5), G2_list, 
      x2pos_list, x3pos_list, x4pos_list, x5pos_list, x6pos_list, x7pos_list, x8pos_list,
      rep(G2[[1]], 35)
    )),
    xB2 = list(c(
      rep(0, 5), x1pos_list, x1neg_list, L2_list, rep(x1neg[[1]], 5), 
      rep(L2[[1]], 35),
      x2neg_list, x3neg_list, x4neg_list, x5neg_list, x6neg_list, x7neg_list, x8neg_list
    )),
    choice = list(c(
      L_choice_list, x1pos_choice_list, x1neg_choice_list, L2_choice_list, G2_choice_list,
      x2pos_choice_list, x3pos_choice_list, x4pos_choice_list, x5pos_choice_list, x6pos_choice_list, x7pos_choice_list, x8pos_choice_list,
      x2neg_choice_list, x3neg_choice_list, x4neg_choice_list, x5neg_choice_list, x6neg_choice_list, x7neg_choice_list, x8neg_choice_list
    ))
  ) %>%
  unnest(c(trial, starts_with("x"), choice)) %>%   # 1 row 1 trial
  ungroup() %>% 
  arrange(subj_id, trial) %>% 
  drop_na() %>% 
  mutate(choice = as.integer(choice == "A"))

# df_long %>% write.csv("./data/preprocessed_data/subj_trial_data.csv", row.names=F)

dim(df_long)
head(df_long)
```

We had 165 subjects, and each subject underwent up to 95 trials.


## Cumulative Prospect Theory (CPT)

The subjective value $V$ of prospect $O$ can be determined by

$$
V(O)=\sum\pi(p_i)v(x_i)
$$

, where $\pi(\cdot)$ is the probability weighting function and $v(\cdot)$ is the value function.

The subjective value of payoff $x$ is defined as

$$
v(x)=
\left\{
  \begin{aligned}
      &x^\alpha, &&\text{if}\ x \geq 0  \\
      &-\lambda(-x)^\beta, &&\text{if}\ x < 0,
  \end{aligned}
\right.
$$

The probability weighting function (Tversky & Kahneman, 1992) considered in only 2 cases can be reduced to

$$
\pi(p_i)=\frac{p_i^c}{[p_i^c-(1-p_i)^c]^{1/c}}
$$

, with $c=\gamma$ for gains and $c=\delta$ for losses.
A probability function is more S-shaped with smaller $c$.
However, the setting of our experiment made every probabilities of outcomes be $0.5$.
Thus, we removed the probability weighting part from our model, and set $\pi(0.5)=0.5$.

The logistic choice rule is

$$
p(A, B)=\frac{1}{1+\exp[\phi(V(B)-V(A))]}
$$

where $\phi>0$ is a sensitivity parameter.
The choice behavior is more deterministic with larger $\phi$.

To sum up, CPT has 6 parameters: $\alpha \in [0, 1]$, $\beta \in [0, 1]$, $\lambda \in (0, \infty)$ and $\phi \in (0, \infty)$.

+ $\alpha$ quantifies the curvature of the subjective value function for gains;
+ $\beta$ quantifies the curvature of the subjective value function for losses;
+ $\lambda$ quantifies loss aversion;
+ $\phi$ quantifies the extent to which choice behavior is guided by subjective values.


### Parameter Setting

Parameters $\alpha_i$ and $\beta_i$ were first transformed to probit scale: 
$\alpha_i^\phi = \Theta^{-1}(\alpha_i)$ and $\beta_i^\phi = \Theta^{-1}(\beta_i)$.
Thus, $\alpha^\phi, \beta^\phi \in \mathbb{R}$.

Next, we assumed these subject-level parameters come from group-level normal distributions: 
$\alpha^\phi_i \sim N(\mu^\alpha, \sigma^\alpha)$ and $\beta^\phi_i \sim N(\mu^\beta, \sigma^\beta)$.

Finally, we decided the priors to the group-level parameters:
$\mu^\alpha \sim N(0, 1)$, $\mu^\beta \sim N(0, 1)$, $\sigma^\alpha \sim U(0, 10)$ and $\sigma^\beta \sim U(0, 10)$.

The two remaining parameters are $\lambda$ and $\phi$, which can only take positive value.
We assumed they come from a lognormal distribution: $\lambda\sim LN(\mu^\lambda, \sigma^\lambda)$ and $\phi\sim LN(\mu^\phi, \sigma^\phi)$.
Since we want $\lambda, \phi \in [0.1, 5]$, the hyper-parameters are $\mu^\lambda\sim U(-2.3, 1.61)$, $\mu^\phi \sim U(-2.3, 1.61)$, $\sigma^\lambda\sim U(0, 1.13)$, and $\sigma^\phi \sim U(0, 1.13)$.


## CPT Fitting with MCMC

We ran MCMC to get the cumulative prospect theory (CPT) parameters.

```{r MCMC for CRRA, cache=T}
# read data
df_long <- read.csv("./data/preprocessed_data/subj_trial_data.csv")

# Setup
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

n <- 20  # take top n subjects
stan_data <- df_long %>%
  filter(subj_id %in% 1:n) %>%
  list(
    N    = nrow(.),
    S    = length(unique(.$subj_id)),
    subj = .$subj_id,
    y    = .$choice,
    xA1  = .$xA1,
    xA2  = .$xA2,
    xB1  = .$xB1,
    xB2  = .$xB2
  )

# Fit model
fit_full <- stan(
  file = "./hierarchical_CPT_no_prob_weight.stan",
  data = stan_data,
  chains = 4,
  iter = 4000,
  warmup = 1000,
  seed = 2025
  # control = list(adapt_delta = 0.95, max_treedepth = 15)
)

fit_rest <- stan(
  file = "./hierarchical_CPT_no_prob_weight_restricted.stan",
  data = stan_data,
  chains = 4,
  iter = 4000,
  warmup = 1000,
  seed = 2025
  # control = list(adapt_delta = 0.95, max_treedepth = 15)
)

saveRDS(fit_full, file="./res/CPT_full-0924.rds")
saveRDS(fit_restricted, file="./res/CPT_restricted-0924.rds")
```


```{r MCMC fit result}
params <- c(
  paste0("alpha[", 1:n, "]"), paste0("beta[", 1:n, "]"),
  paste0("lambda[", 1:n, "]"), paste0("phi[", 1:n, "]")
)

print(fit, pars = params)

# mcmc_trace(fit, pars = params) +
  # labs(title = "Trace plot")

# mcmc_dens_overlay(fit, pars = params) +
  # labs(title = "Posterior Densities")

# mcmc_intervals(
  # fit, pars = params, 
  # point_est="mean", prob=.5, prob_outer = .95
# ) +
  # geom_vline(xintercept=1, color="#AB1212") +
  # labs(title = "Posterior Intervals")
```


