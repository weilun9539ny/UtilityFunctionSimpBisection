---
title: "Stage 1 Utility Function Estimation"
author: "Wei-Lun, Lin"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(magrittr)
library(stringr)
library(tidyverse)

library(rstan)
library(bayesplot)
library(posterior)
library(loo)
```


## Data Reading and Processing

We first get raw data from folders of 2021, 2022 and 2023.
The format of the data in 2021 is a little different from those in 2022 and 2023, 
so we need to separate different cases.

```{r get rawdata}
# get rawdata from foldes of 2021-2023
rawdata <- data.frame()
files.dir <- list.files("./raw data", full.names=T)[-16]
for (i in 1:length(files.dir)) {
  path <- paste0(files.dir[i], "/rawData")
  files <- list.files(path, pattern=".csv", full.names=T)
  for (file in files) {
    if (i < 7) sub.data <- read.csv(file)[, 1:247]
    else sub.data <- read.csv(file)[, 2:248]
    sub <- str_split(file, "/")[[1]][5] %>% substr(1, 10)
    sub.data$sub <- sub
    # print(sub)
    rawdata <- rbind(rawdata, sub.data)
  }
}

head(rawdata)
```

The extracted raw data contains action time points, values and choices.
To note that, we do not need the time point data, so we remove them next.

```{r remove RT and add subj id}
col.target <- c(
  seq(3, 247, 13), 
  seq(4, 247, 13)
) %>% 
  sort()
colnames(rawdata)[248] <- "subject"
rawdata <- rawdata[, c(248, col.target)]
rawdata <- rawdata %>% 
  mutate(
    subj_id = row_number()
  ) %>% 
  relocate(subj_id)

head(rawdata)
```

Next, the cells in the dataset are strings of python list.
For example, a possible element could be `"[1000, 800, 600, 400, 200, 100, None, None, 100]"`,
or `"['A', 'B', 'B', 'A', 'A']"`.
Thus, the strings should be convert into vector, and the excess information (the `100, None, None, 100`) should be removed.
Finally, make the data be in long format, which helps further analysis.

I use `fromJSON()` in package `jsonlite` to parse the string of list, and convert elements into vector.
In this way, the data type could be automatically be determined by parser.

```{r string parse}
parse_list_string <- function(x) {
  # replace '' and None
  x <- gsub("None", "null", x)
  x <- gsub("'", '"', x)
  # parse the string
  res <- jsonlite::fromJSON(x)
  res <- if(length(res) == 0) NA else res
  return(res)
}

remove_excess_info <- function(vec) {
  vec[1:5]
}

rawdata <- rawdata %>% 
  mutate(
    across(-c(subj_id, subject), ~map(.x, parse_list_string)),  # make python list into R vector
    across(ends_with("_list"), ~map(.x, remove_excess_info))  # make value list length be 5
  ) %>% 
  rowwise() %>% 
  mutate(trial = list(1:5)) %>% 
  relocate(trial, .after=subject) %>% 
  unnest(c(trial, ends_with("_list"), ends_with("_choice_list"))) %>% 
  ungroup()

write.csv(rawdata, "./rawdata.csv", row.names=F)
head(rawdata)
```



And we transform the data to be long data.

```{r raw 2 long}

```




## Analysis

We ran MCMC to get the cumulative prospect theory (CPT) parameters.

```{r}
# Setup
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

stan_data <- list(
  N   = nrow(df),
  S   = length(unique(df$subject_id)),
  subj = df$subject_id,
  y    = as.integer(df$choice),
  xA1  = as.numeric(df$xA1),
  xA2  = as.numeric(df$xA2),
  pA   = as.numeric(df$pA),
  xB1  = as.numeric(df$xB1),
  xB2  = as.numeric(df$xB2),
  pB   = as.numeric(df$pB)
)

# Fit model
fit <- stan(
  file = "./hierarchical_CPT.stan",
  data = stan_data,
  chains = 4,
  iter = 2000,
  warmup = 1000,
  seed = 2025,
  control = list(adapt_delta = 0.95, max_treedepth = 15)
)
```



```{r}
library(bayesplot)
library(posterior)

# Make sure rstan uses multiple cores
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

# Data preparation (example)
# df should include: subject, choice (0/1), xA1, xA2, pA, xB1, xB2, pB

df <- df %>% 
  mutate(subject_id = as.integer(factor(subject)))

df$subject_id <- as.integer(factor(df$subject))
stan_data <- list(
  N = nrow(df),
  S = length(unique(df$subject_id)),
  subj = df$subject_id,
  y = as.integer(df$choice),
  xA1 = as.numeric(df$xA1),
  xA2 = as.numeric(df$xA2),
  pA = as.numeric(df$pA),
  xB1 = as.numeric(df$xB1),
  xB2 = as.numeric(df$xB2),
  pB = as.numeric(df$pB)
)

# Compile and sample
fit <- stan(
  file = "cpt_hierarchical.stan",  # path to Stan file (the code you have)
  data = stan_data,
  chains = 4,
  iter = 2000,
  warmup = 1000,
  seed = 1234,
  control = list(adapt_delta = 0.95, max_treedepth = 15)
)

print(fit, pars = c("mu_alpha","mu_beta","mu_gamma","mu_delta","mu_loglambda","mu_logphi"))

# Posterior extraction
posterior_draws <- extract(fit)

# Posterior predictive check
log_lik <- extract_log_lik(fit, parameter_name = "log_lik")
loo_result <- loo::loo(log_lik)
print(loo_result)
```






